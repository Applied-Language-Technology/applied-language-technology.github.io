{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"data/GUM_whow_parachute.conllu\", \"r\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(conllu.parse_incr(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the type of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conllu.models.TokenList"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discourse segment ID can be retrieved from the `metadata` attribute of a *TokenList* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'newdoc id': 'GUM_whow_parachute',\n",
       " 'sent_id': 'GUM_whow_parachute-1',\n",
       " 'text': 'How to Cope With a Double Parachute Failure',\n",
       " 's_type': 'inf'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discourse annotations are always contained under the key `misc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# newdoc id = GUM_whow_parachute\n",
      "# sent_id = GUM_whow_parachute-1\n",
      "# text = How to Cope With a Double Parachute Failure\n",
      "# s_type = inf\n",
      "1\tHow\thow\tSCONJ\tWRB\tPronType=Int\t3\tmark\t_\tDiscourse=preparation:1->11\n",
      "2\tto\tto\tPART\tTO\t_\t3\tmark\t_\t_\n",
      "3\tCope\tCope\tVERB\tVB\tVerbForm=Inf\t0\troot\t_\t_\n",
      "4\tWith\twith\tADP\tIN\t_\t8\tcase\t_\t_\n",
      "5\ta\ta\tDET\tDT\tDefinite=Ind|PronType=Art\t8\tdet\t_\tEntity=(event-1\n",
      "6\tDouble\tdouble\tADJ\tJJ\tDegree=Pos\t8\tamod\t_\t_\n",
      "7\tParachute\tparachute\tNOUN\tNN\tNumber=Sing\t8\tcompound\t_\tEntity=(object-2)\n",
      "8\tFailure\tfailure\tNOUN\tNN\tNumber=Sing\t3\tobl\t_\tEntity=event-1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0].serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the *TokenList* objects into spaCy *Docs*: https://spacy.io/api/doc\n",
    "\n",
    "Then register custom attributes for RST and sentence type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training.converters import conllu_to_docs\n",
    "from spacy.tokens.span_group import SpanGroup\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy.vocab import Vocab\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anticipating the need to store discourse annotations into spaCy *Doc* and *Span* objects, we register several custom attributes with spaCy as instructed in [Part II](../part_ii/04_basic_nlp_continued.html#adding-custom-attributes-to-spacy-objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register custom attributes named 'sentence_type' and 'sentence_id' \n",
    "# for Doc object; set default value to None for both\n",
    "Doc.set_extension('sentence_type', default=None)\n",
    "Doc.set_extension('sentence_id', default=None)\n",
    "\n",
    "# Register custom attributes named 'edu_id'\n",
    "Span.set_extension('edu_id', default=None)\n",
    "Span.set_extension('target_id', default=None)\n",
    "Span.set_extension('relation', default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_doc(nlp_object, token_list):\n",
    "    \n",
    "    # Assert the input is of correct type, e.g. a TokenList\n",
    "    assert type(token_list) == conllu.models.TokenList\n",
    "    \n",
    "    # Collect the form of tokens from the TokenList object\n",
    "    words = [token['form'] for token in token_list]\n",
    "    \n",
    "    # Collect information on whether token should be followed by empty space\n",
    "    spaces = [False if token['misc'] is not None \n",
    "              and 'SpaceAfter' in token['misc'] \n",
    "              and token['misc']['SpaceAfter'] == \"No\"\n",
    "              else True for token in token_list]\n",
    "    \n",
    "    # Create a spaCy Doc object manually\n",
    "    doc = Doc(vocab=nlp.vocab, words=words, spaces=spaces)\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_conllu_to_doc(sentence):\n",
    "    \n",
    "    # Assert the input is of correct type, e.g. a TokenList\n",
    "    assert type(sentence) == conllu.models.TokenList\n",
    "    \n",
    "    # Next, we convert the TokenList object into a spaCy Doc object\n",
    "    doc = tokens_to_doc(nlp, sentence)\n",
    "    \n",
    "    # Pass the Doc object through the pipeline\n",
    "    for name, processor in nlp.pipeline:\n",
    "        \n",
    "        doc = processor(doc)\n",
    "    \n",
    "    # Next, we extract the sentence ID and type from the conllu TokenList\n",
    "    # object and assign them to variables 'sent_id' and 'sent_type'.\n",
    "    sent_id = sentence.metadata['sent_id']\n",
    "    sent_type = sentence.metadata['s_type']\n",
    "        \n",
    "    # We then create placeholder list for holding Token indices that mark\n",
    "    # the boundaries of elementary discourse units, their identifiers and \n",
    "    # discourse relations\n",
    "    edus = []\n",
    "    edu_ids = []\n",
    "    target_ids = []\n",
    "    disc_rels = []\n",
    "    \n",
    "    # Next, we loop over each item (Token) in the TokenList object 'sentence'\n",
    "    # to examine its attributes.\n",
    "    for token in sentence:\n",
    "                        \n",
    "        # Check if the 'misc' dictionary under Token exists and contains the \n",
    "        # key 'Discourse'.\n",
    "        if token['misc'] is not None and 'Discourse' in token['misc']:\n",
    "            \n",
    "            # If both conditions are true, append token index to the 'discourse_units'\n",
    "            # list that holds the indices of discourse unit boundaries.\n",
    "            edus.append(sentence.index(token))\n",
    "            \n",
    "            # Get the discourse relation definition stored under the key 'Discourse'\n",
    "            rel_definition = token['misc']['Discourse']\n",
    "            \n",
    "            # The relation definitions are provided as strings with the following pattern:\n",
    "            #\n",
    "            #  preparation:1->11\n",
    "            #\n",
    "            # The relation name is on the left-hand side of a colon, whereas the \n",
    "            # right-hand side contains identifiers for the participating units. \n",
    "            # The first identifier is always the identifier for the current unit. \n",
    "            # The second is the \"target\" of the relation. To separate them, we \n",
    "            # use the split() method to split at colon.\n",
    "            rel, rel_ids = rel_definition.split(':')\n",
    "            \n",
    "            # We then check if the 'rel_ids' string contains '->' indicating a \n",
    "            # relation. The root element does not have this.\n",
    "            if '->' in rel_ids:\n",
    "                \n",
    "                # Get the unit and target identifiers by splitting at '->'\n",
    "                edu_id, target_edu_id = rel_ids.split('->')\n",
    "                \n",
    "                # Add EDU and target EDU ids to list\n",
    "                edu_ids.append(edu_id)\n",
    "                target_ids.append(target_edu_id)\n",
    "            \n",
    "            # Define alternative steps for processing root elements\n",
    "            else:\n",
    "                \n",
    "                # Add root element identifier to lists\n",
    "                edu_ids.append(rel_ids)\n",
    "                \n",
    "                # Add target EDU id to list as None\n",
    "                target_ids.append(None)\n",
    "                \n",
    "            # Add discourse relation to list\n",
    "            disc_rels.append(rel)\n",
    "            \n",
    "    # Finally, append the length of the TokenList object to the 'discourse_units' list \n",
    "    # to mark the boundary of the final discourse unit!\n",
    "    edus.append(len(sentence))\n",
    "    \n",
    "    # Next, we create a placeholder list to hold slices of the Doc object that correspond\n",
    "    # to discourse units.\n",
    "    edu_spans = []\n",
    "\n",
    "    # Proceed to loop over the discourse units. To do so, we use Python's range() function, \n",
    "    # to count from zero to the length of the 'edus' list minus one. This is  because the \n",
    "    # final item will never mark the beginning of a discourse unit. We use these numbers \n",
    "    # as indices for list items during the following loop!\n",
    "    for edu in range(len(edus) - 1):\n",
    "        \n",
    "        # Get the start of the discourse unit by fetching the current item in the list\n",
    "        start = edus[edu]\n",
    "        \n",
    "        # Get the end of the discourse unit by fetching the next item in the list\n",
    "        end = edus[edu + 1]\n",
    "        \n",
    "        # The 'start' and 'end' variables now hold Token indices; use them to slice the\n",
    "        # spaCy Doc object 'doc' into Span objects. Add Spans to list 'edu_spans'.\n",
    "        edu_spans.append(doc[start:end])\n",
    "        \n",
    "    # This is a good point to check that the number of EDUs matches the number of identifiers\n",
    "    # and relations. This will throw an error if something goes wrong.\n",
    "    assert len(edu_spans) == len(edu_ids) == len(target_ids) == len(disc_rels)\n",
    "    \n",
    "    # Next we create a spaCy SpanGroup object that holds all our Spans for elementary discourse\n",
    "    # units. The 'doc' argument takes the Doc object that the Spans belong to as input, whereas\n",
    "    # 'name' defines the key used to retrieve the spans from the 'spans' attribute of the Doc\n",
    "    # object. Finally, 'spans' takes a list of Span objects to be included in the Span group.\n",
    "    span_group = SpanGroup(doc=doc, name=\"edus\", spans=edu_spans)\n",
    "    \n",
    "    # Assign the SpanGroup to the 'spans' attribute of the Doc object under the key 'edus'\n",
    "    doc.spans['edus'] = span_group\n",
    "    \n",
    "    for i, span in enumerate(doc.spans['edus']):\n",
    "        \n",
    "        span._.edu_id = edu_ids[i]\n",
    "        span._.target_id = target_ids[i]\n",
    "        span._.relation = disc_rels[i]\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "discourse = [convert_conllu_to_doc(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E873] Unable to merge a span from doc.spans with key 'edus' and text 'While skydiving,'. This is likely a bug in spaCy, so feel free to open an issue: https://github.com/explosion/spaCy/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a97fb00a043f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscourse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/nlp/lib/python3.8/site-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.from_docs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E873] Unable to merge a span from doc.spans with key 'edus' and text 'While skydiving,'. This is likely a bug in spaCy, so feel free to open an issue: https://github.com/explosion/spaCy/issues"
     ]
    }
   ],
   "source": [
    "docs = Doc.from_docs(discourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.spans['edus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"GUM_whow_parachute.conllu\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "conllu = data.read()\n",
    "\n",
    "docs = list(conllu_to_docs(conllu, no_print=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New plan\n",
    "\n",
    "- use `conllu` to extract metadata, then use `TokenList.serialize()` and feed this to spaCy `conllu_to_docs` to create a Doc object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TokenList into a CoNLL-U compliant string object using\n",
    "# the serialize() method. Then feed this string to the conllu_to_docs()\n",
    "# function from spaCy. Setting the no_print argument to True prevents \n",
    "# any output at this stage.\n",
    "# doc = list(conllu_to_docs(sentence.serialize(), no_print=True))\n",
    "\n",
    "# Because the conllu_to_docs() function returns a Python generator object, we\n",
    "# must cast the output into a list to examine its contents. We then access \n",
    "# the first item at index 0 to access the Doc object and update the variable.\n",
    "# doc = doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = nlp(\"This is a test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1[0].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
