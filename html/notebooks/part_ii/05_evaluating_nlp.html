
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QDXYHY09XJ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QDXYHY09XJ');
</script>
    
    
    <title>Evaluating language models</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Managing textual data using pandas" href="06_managing_data.html" />
    <link rel="prev" title="Customising the spaCy pipeline" href="04_basic_nlp_continued.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about.html">
   About this Website
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../getting_started.html">
   Getting Started
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebook_login.html">
     Log in to CSC Notebooks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../launch_server.html">
     Launch JupyterLab on your personal server
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../jupyter.html">
     Interact with the server in JupyterLab
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../github_pull.html">
     Retrieve learning materials and exercises from GitHub
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../github_push.html">
     Return completed exercises to GitHub for grading
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../python_intro.html">
   Part I: A Minimal Introduction to Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_i/01_working_with_jupyter_notebooks.html">
     The elements of a Jupyter Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_i/02_getting_started_with_python.html">
     Getting started with Python
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../../working_with_text.html">
   Part II: Working with Text in Python
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01_basic_text_processing.html">
     Manipulating text using Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_basic_text_processing_continued.html">
     Manipulating text at scale
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_basic_nlp.html">
     Processing texts using spaCy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04_basic_nlp_continued.html">
     Customising the spaCy pipeline
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Evaluating language models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06_managing_data.html">
     Managing textual data using pandas
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../nlp_for_linguists.html">
   Part III: Natural Language Processing for Linguists
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_iii/01_multilingual_nlp.html">
     Processing diverse languages
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_iii/02_universal_dependencies.html">
     Universal Dependencies
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_iii/03_pattern_matching.html">
     Finding linguistic patterns using spaCy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_iii/04_embeddings.html">
     Introducing word embeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_iii/05_embeddings_continued.html">
     Word embeddings in spaCy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_iii/06_text_linguistics.html">
     Working with discourse-level annotations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../citation.html">
   Citation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../resources.html">
   Resources
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/notebooks/part_ii/05_evaluating_nlp.ipynb.txt"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Applied-Language-Technology/website"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/Applied-Language-Technology/website/binder?urlpath=lab/tree/notebooks/part_ii/05_evaluating_nlp.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-a-gold-standard">
   What is a gold standard?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#measuring-reliability-manually">
   Measuring reliability manually
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-1-annotate-data">
     Step 1: Annotate data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-2-calculate-percentage-agreement">
     Step 2: Calculate percentage agreement
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-3-calculate-probabilities-for-each-category">
     Step 3: Calculate probabilities for each category
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-4-estimate-chance-agreement">
     Step 4: Estimate chance agreement
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cohen-s-kappa-as-a-measure-of-agreement">
   Cohen’s kappa as a measure of agreement
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluating-the-performance-of-language-models">
   Evaluating the performance of language models
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="evaluating-language-models">
<h1>Evaluating language models<a class="headerlink" href="#evaluating-language-models" title="Permalink to this headline">¶</a></h1>
<p>This section introduces you to some basic techniques for evaluating the results of natural language processing.</p>
<p>After reading this section, you should:</p>
<ul class="simple">
<li><p>understand what is meant by a gold standard</p></li>
<li><p>know how to evaluate agreement between human annotators</p></li>
<li><p>understand simple metrics for evaluating the performance of natural language processing</p></li>
</ul>
<div class="section" id="what-is-a-gold-standard">
<h2>What is a gold standard?<a class="headerlink" href="#what-is-a-gold-standard" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe
    width="600"
    height="350"
    src="https://www.youtube.com/embed/eBJDxHUxRwc"
    frameborder="0"
    allowfullscreen

></iframe>
</div></div>
</div>
<p>A gold standard – also called a ground truth – refers to human-verified data that can used as a benchmark for evaluating the performance of algorithms.</p>
<p>In natural language processing, gold standards are used to measure how well humans perform on some task.</p>
<p>The goal of natural language processing is to allow computers to achieve or surpass human-level performance in some pre-defined task.</p>
<p>Measuring whether algorithms can do so requires a benchmark, which is provided by the gold standard. Put simply, a gold standard provides a point of reference.</p>
<p>It is important, however, to understand that gold standards are <em>abstractions</em> of language use.</p>
<p>Consider, for instance, the task of placing words into word classes: word classes are not given to us by nature, but represent an abstraction that imposes structure on natural language.</p>
<p>Language, however, is naturally ambiguous and subjective, and the abstractions used can be underdeveloped – we cannot be sure if all users would categorise words in the same way.</p>
<p>This is why we need to measure the reliability of any gold standard, that is, to what extent humans agree on the task.</p>
</div>
<div class="section" id="measuring-reliability-manually">
<h2>Measuring reliability manually<a class="headerlink" href="#measuring-reliability-manually" title="Permalink to this headline">¶</a></h2>
<p>This section introduces how reliability, often understood as agreement between multiple annotators, can be measured manually.</p>
<div class="section" id="step-1-annotate-data">
<h3>Step 1: Annotate data<a class="headerlink" href="#step-1-annotate-data" title="Permalink to this headline">¶</a></h3>
<p>Sentiment analysis is a task that involves determining the sentiment of a text (for an useful overview that incorporates insights from both linguistics and natural language processing, see <a class="reference external" href="https://doi.org/10.1146/annurev-linguistics-011415-040518">Taboada</a> (2016).</p>
<p>Training a sentiment analysis model requires collecting training data, that is, examples of texts associated with different sentiments.</p>
<p>Classify the following tweets into three categories – <em>positive</em>, <em>neutral</em> or <em>negative</em> – based on their sentiment.</p>
<p>Write down your decision – one per row – but <strong>do not discuss them or show them to the person next to you.</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Updated: HSL GTFS (Helsinki, Finland) https://t.co/fWEpzmNQLz
2. current weather in Helsinki: broken clouds, -8°C 100% humidity, wind 4kmh, pressure 1061mb
3. CNN: &quot;WallStreetBets Redditors go ballistic over GameStop&#39;s sinking share price&quot;
4. Baana bicycle counter. Today: 3 Same time last week: 1058 Trend: ↓99% This year: 819 518 Last year: 802 079 #Helsinki #cycling
5. Elon Musk is now tweeting about #bitcoin 
6. A perfect Sunday walk in the woods just a few steps from home.
7. Went to Domino&#39;s today👍 It was so amazing and I think I got damn good dessert as well…
8. Choo Choo 🚂 There&#39;s our train! 🎉 #holidayahead
9. Happy women&#39;s day ❤️💋 kisses to all you beautiful ladies. 😚 #awesometobeawoman
10. Good morning #Helsinki! Sun will rise in 30 minutes (local time 07:28)
</pre></div>
</div>
<p>Double-click this cell to edit its contents and write down your classifications below:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
</pre></div>
</div>
</div>
<div class="section" id="step-2-calculate-percentage-agreement">
<h3>Step 2: Calculate percentage agreement<a class="headerlink" href="#step-2-calculate-percentage-agreement" title="Permalink to this headline">¶</a></h3>
<p>When creating datasets for training models, we typically want the training data to be reliable, that is, so that we agree on whatever we are describing – in this case, the sentiment of the tweets above.</p>
<p>One way to measure this is simple <em>percentage agreement</em>, that is, how many times out of 10 you and the person next to you agreed on the sentiment of a tweet.</p>
<p>Now compare your results calculate percentage agreement by dividing the number of times you agreed by the number of items (10).</p>
<p>You can calculate percentage agreement by executing the cell below: just assign the number items you agree on to the variable <code class="docutils literal notranslate"><span class="pre">agreement</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Replace this number here with the number of items you agreed on</span>
<span class="n">agreement</span> <span class="o">=</span> <span class="mi">0</span>  

<span class="c1"># Divide the count by the number of tweets</span>
<span class="n">agreement</span> <span class="o">=</span> <span class="n">agreement</span> <span class="o">/</span> <span class="mi">10</span>

<span class="c1"># Print out the variable</span>
<span class="n">agreement</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="step-3-calculate-probabilities-for-each-category">
<h3>Step 3: Calculate probabilities for each category<a class="headerlink" href="#step-3-calculate-probabilities-for-each-category" title="Permalink to this headline">¶</a></h3>
<p>Percentage agreement is actually a very poor measure of agreement, as either of you may have made lucky guesses – or perhaps you considered the task boring and classified every tweet into a random category.</p>
<p>If you did, we have no way of knowing this, as percentage agreement cannot tell us if the result occurred by chance!</p>
<p>Luckily, we can estimate the possibility of <em>chance agreement</em> easily.</p>
<p>The first step is to count <em>how many times you used each available category</em> (positive, neutral or negative).</p>
<p>Assign these counts in the variables below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Count how many items *you* placed in each category</span>
<span class="n">positive</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">neutral</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">negative</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
<p>We can convert these counts into <em>probabilities</em> by dividing them with the total number of tweets classified.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">positive</span> <span class="o">=</span> <span class="n">positive</span> <span class="o">/</span> <span class="mi">10</span>
<span class="n">neutral</span> <span class="o">=</span> <span class="n">neutral</span> <span class="o">/</span> <span class="mi">10</span>
<span class="n">negative</span> <span class="o">=</span> <span class="n">negative</span> <span class="o">/</span> <span class="mi">10</span>

<span class="c1"># Call each variable to examine the output</span>
<span class="n">positive</span><span class="p">,</span> <span class="n">neutral</span><span class="p">,</span> <span class="n">negative</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.0, 0.0, 0.0)
</pre></div>
</div>
</div>
</div>
<p>These probabilities represent the chance of <em>you</em> choosing that particular category.</p>
<p>Now ask the person sitting next to you for their corresponding probabilities and tell them yours as well.</p>
<p>Add their probabilities to the variables below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb_positive</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">nb_neutral</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">nb_negative</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we know the probabilities for each class for both annotators, we can calculate the probability that both annotators choose the same category by chance.</p>
<p>This is easy: for each category, simply multiply your probability with the corresponding probability from the person next to you.</p>
<p>If either annotator did not assign a single tweet into a category, e.g. negative, and the other annotator did, then this effectively rules out the possibility of agreeing by chance (multiplication by zero results in zero).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">both_positive</span> <span class="o">=</span> <span class="n">positive</span> <span class="o">*</span> <span class="n">nb_positive</span>
<span class="n">both_neutral</span> <span class="o">=</span> <span class="n">neutral</span> <span class="o">*</span> <span class="n">nb_neutral</span>
<span class="n">both_negative</span> <span class="o">=</span> <span class="n">negative</span> <span class="o">*</span> <span class="n">nb_negative</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="step-4-estimate-chance-agreement">
<h3>Step 4: Estimate chance agreement<a class="headerlink" href="#step-4-estimate-chance-agreement" title="Permalink to this headline">¶</a></h3>
<p>Now we are ready to calculate how likely you are to agree by chance.</p>
<p>This is known as <em>expected agreement</em>, which is calculated by summing up your combined probabilities for each category.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">expected_agreement</span> <span class="o">=</span> <span class="n">both_positive</span> <span class="o">+</span> <span class="n">both_neutral</span> <span class="o">+</span> <span class="n">both_negative</span>

<span class="n">expected_agreement</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
</div>
</div>
<p>Now that we know both observed percentage agreement (<code class="docutils literal notranslate"><span class="pre">agreement</span></code>) and the agreement expected by chance (<code class="docutils literal notranslate"><span class="pre">expected_agreement</span></code>), we can use this information for a more reliable measure of <em>agreement</em>.</p>
<p>One such measure is <a class="reference external" href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen’s kappa</a> (<span class="math notranslate nohighlight">\(\kappa\)</span>), which estimates agreement on the basis of both observed and expected agreement.</p>
<p>The formula for Cohen’s <span class="math notranslate nohighlight">\(\kappa\)</span> is as follows:</p>
<p><span class="math notranslate nohighlight">\(\kappa = \frac{P_{observed} - P_{expected}}{1 - P_{expected}}\)</span></p>
<p>As all this information is stored in our variables <code class="docutils literal notranslate"><span class="pre">agreement</span></code> and <code class="docutils literal notranslate"><span class="pre">expected_agreement</span></code>, we can easily count the <span class="math notranslate nohighlight">\(\kappa\)</span> score using the code below.</p>
<p>Note that we must wrap the subtractions into parentheses to perform them before division.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kappa</span> <span class="o">=</span> <span class="p">(</span><span class="n">agreement</span> <span class="o">-</span> <span class="n">expected_agreement</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">expected_agreement</span><span class="p">)</span>

<span class="n">kappa</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
</div>
</div>
<p>This gives us the result for Cohen’s <span class="math notranslate nohighlight">\(\kappa\)</span>.</p>
<p>Let’s now consider how to interpret its value.</p>
</div>
</div>
<div class="section" id="cohen-s-kappa-as-a-measure-of-agreement">
<h2>Cohen’s kappa as a measure of agreement<a class="headerlink" href="#cohen-s-kappa-as-a-measure-of-agreement" title="Permalink to this headline">¶</a></h2>
<p>The theoretical value Cohen’s <span class="math notranslate nohighlight">\(\kappa\)</span> runs from <span class="math notranslate nohighlight">\(-1\)</span> indicating perfect disagreement to <span class="math notranslate nohighlight">\(+1\)</span> for perfect agreement, with <span class="math notranslate nohighlight">\(0\)</span> standing for completely random agreement.</p>
<p>The <span class="math notranslate nohighlight">\(\kappa\)</span> score is often interpreted as a measure of the strength of agreement.</p>
<p><a class="reference external" href="https://doi.org/10.2307/2529310">Landis and Koch</a> (1977) famously proposed the following benchmarks, which should nevertheless be taken with a pinch of salt as the divisions are completely arbitrary.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Cohen’s K</p></th>
<th class="head"><p>Strength of agreement</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>&lt;0.00</p></td>
<td><p>Poor</p></td>
</tr>
<tr class="row-odd"><td><p>0.00–0.20</p></td>
<td><p>Slight</p></td>
</tr>
<tr class="row-even"><td><p>0.21–0.40</p></td>
<td><p>Fair</p></td>
</tr>
<tr class="row-odd"><td><p>0.41–0.60</p></td>
<td><p>Moderate</p></td>
</tr>
<tr class="row-even"><td><p>0.61–0.80</p></td>
<td><p>Substantial</p></td>
</tr>
<tr class="row-odd"><td><p>0.81–1.00</p></td>
<td><p>Almost perfect</p></td>
</tr>
</tbody>
</table>
<p>Cohen’s <span class="math notranslate nohighlight">\(\kappa\)</span> can be used to measure agreement between <strong>two</strong> annotators and the categories available must be <strong>fixed</strong> in advance.</p>
<p>For measuring agreement between more than two annotators, one must use a measure such as <a class="reference external" href="https://en.wikipedia.org/wiki/Fleiss%27_kappa">Fleiss’</a> <span class="math notranslate nohighlight">\(\kappa\)</span>.</p>
<p>Cohen’s <span class="math notranslate nohighlight">\(\kappa\)</span> and many more measures of agreement are implemented in various Python libraries, so one rarely needs to perform the calculations manually.</p>
<p>The <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score">scikit-learn</a> library (<code class="docutils literal notranslate"><span class="pre">sklearn</span></code>), for instance, includes an implementation of Cohen’s <span class="math notranslate nohighlight">\(\kappa\)</span>.</p>
<p>Let’s import the <code class="docutils literal notranslate"><span class="pre">cohen_kappa_score()</span></code> function for calculating Cohen’s <span class="math notranslate nohighlight">\(\kappa\)</span> from scikit-learn.</p>
<p>This function takes two <em>lists</em> as input and calculates the <span class="math notranslate nohighlight">\(\kappa\)</span> score between them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the cohen_kappa_score function from the &#39;metrics&#39; module of the scikit-learn library</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">cohen_kappa_score</span>
</pre></div>
</div>
</div>
</div>
<p>We can then define two lists of part-of-speech tags, which make up our toy example for calculating Cohen’s <span class="math notranslate nohighlight">\(\kappa\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define two lists named &#39;a1&#39; and &#39;a2&#39;</span>
<span class="n">a1</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;AUX&#39;</span><span class="p">,</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">]</span>
<span class="n">a2</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">,</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The next step is to feed the two lists, <code class="docutils literal notranslate"><span class="pre">a1</span></code> and <code class="docutils literal notranslate"><span class="pre">a2</span></code>, to the <code class="docutils literal notranslate"><span class="pre">cohen_kappa_score()</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use the cohen_kappa_score() function to calculate agreement between the lists</span>
<span class="n">cohen_kappa_score</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.44444444444444453
</pre></div>
</div>
</div>
</div>
<p>According to the benchmark from Landis and Koch, this score would indicate moderate agreement.</p>
<p>Generally, Cohen’s <span class="math notranslate nohighlight">\(\kappa\)</span> can be used for measuring agreement on all kinds of tasks that involve placing items into categories.</p>
<p>It is rarely necessary to annotate the whole dataset when measuring agreement – a random sample is often enough.</p>
<p>If Cohen’s <span class="math notranslate nohighlight">\(\kappa\)</span> suggests that the human annotators agree on whatever they are categorising, we can assume that the annotations are <em>reliable</em> in the sense that they are not random.</p>
<p>However, all measures of inter-annotator agreement, Cohen’s <span class="math notranslate nohighlight">\(\kappa\)</span> included, are affected by their underlying assumptions about what agreement is and how it is calculated. In other words, these measures never represent the absolute truth (see e.g. Di Eugenio &amp; Glass <a class="reference external" href="https://dx.doi.org/10.1162/089120104773633402">2004</a>; Artstein &amp; Poesio <a class="reference external" href="https://doi.org/10.1162/coli.07-034-R2">2008</a>).</p>
</div>
<div class="section" id="evaluating-the-performance-of-language-models">
<h2>Evaluating the performance of language models<a class="headerlink" href="#evaluating-the-performance-of-language-models" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe
    width="600"
    height="350"
    src="https://www.youtube.com/embed/WiN5JCueeFQ"
    frameborder="0"
    allowfullscreen

></iframe>
</div></div>
</div>
<p>Once we have a sufficiently <em>reliable</em> gold standard, we can use the gold standard to measure the performance of language models.</p>
<p>Let’s assume that we have a reliable gold standard consisting of 10 tokens annotated for their part-of-speech tags.</p>
<p>These part-of-speech tags are given in the list <code class="docutils literal notranslate"><span class="pre">gold_standard</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a list named &#39;gold_standard&#39;</span>
<span class="n">gold_standard</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;AUX&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">,</span> <span class="s1">&#39;AUX&#39;</span><span class="p">,</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;DET&#39;</span><span class="p">,</span> <span class="s1">&#39;PRON&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We then retrieve the predictions for the same tokens from some language model and store them in a list named <code class="docutils literal notranslate"><span class="pre">predictions</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a list named &#39;predictions&#39;</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;AUX&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">,</span> <span class="s1">&#39;AUX&#39;</span><span class="p">,</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">,</span> <span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;DET&#39;</span><span class="p">,</span> <span class="s1">&#39;PROPN&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have a toy data set with two sets of annotations to compare, let’s import the entire <em>metrics</em> module from the <em>scikit-learn</em> library and apply them to our data.</p>
<p>This module contains implementations for <a class="reference external" href="https://scikit-learn.org/stable/modules/model_evaluation.html">various evaluation metrics</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the &#39;metrics&#39; module from the scikit-learn library (sklearn)</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
</pre></div>
</div>
</div>
</div>
<p>First of all, we can calculate <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score">accuracy</a> using the <code class="docutils literal notranslate"><span class="pre">accuracy_score()</span></code> function, which is precisely the same as observed agreement that we calculated manually above.</p>
<p>This function takes two lists as input.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use the accuracy_score() function from the &#39;metrics&#39; module</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">gold_standard</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7
</pre></div>
</div>
</div>
</div>
<p>Accuracy, however, suffers from the same shortcoming as observed agreement – the output of the language model in <code class="docutils literal notranslate"><span class="pre">predictions</span></code> may be the result of making a series of lucky guesses.</p>
<p>However, given that we are working with a toy example, we can easily verify that 7 out of 10 part-of-speech tags match. This gives an accuracy of 0.7 or 70%.</p>
<p>To better evaluate the performance of the language model against our gold standard, the results can be organised into what is called a <em>confusion matrix</em>.</p>
<p>To do so, we need all part-of-speech tags that occur in <code class="docutils literal notranslate"><span class="pre">gold_standard</span></code> and <code class="docutils literal notranslate"><span class="pre">predictions</span></code>.</p>
<p>We can easily collect unique part-of-speech tags using the <code class="docutils literal notranslate"><span class="pre">set()</span></code> function.</p>
<p>The result is a <em>set</em>, a powerful data structure in Python, which consists of a collection of unique items.</p>
<p>Essentially, we use a set to remove duplicates in the two lists <code class="docutils literal notranslate"><span class="pre">gold_standard</span></code> and <code class="docutils literal notranslate"><span class="pre">predictions</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Collect unique POS tags into a set by combining the two lists</span>
<span class="n">pos_tags</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">gold_standard</span> <span class="o">+</span> <span class="n">predictions</span><span class="p">)</span>

<span class="c1"># Sort the set alphabetically and cast the result into a list</span>
<span class="n">pos_tags</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">pos_tags</span><span class="p">))</span>

<span class="c1"># Print the resulting list</span>
<span class="n">pos_tags</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;ADJ&#39;, &#39;AUX&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;PRON&#39;, &#39;PROPN&#39;, &#39;VERB&#39;]
</pre></div>
</div>
</div>
</div>
<p>We can use these unique categories to compile a table, in which the rows stand for the gold standard and the columns stand for predictions made by the language model. Having collected all unique part-of-speech tags at hand ensures that we can always find a place for each item.</p>
<p>This kind of table is commonly called a <em>confusion matrix</em>.</p>
<p>The table is populated by simply walking through each pair of items in the gold standard and model predictions, adding <span class="math notranslate nohighlight">\(+1\)</span> to the cell for this combination.</p>
<p>For example, the first item in <code class="docutils literal notranslate"><span class="pre">gold_standard</span></code> is ADJ, whereas the first item in <code class="docutils literal notranslate"><span class="pre">predictions</span></code> is NOUN.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print out the first item in each list</span>
<span class="n">gold_standard</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&#39;ADJ&#39;, &#39;NOUN&#39;)
</pre></div>
</div>
</div>
</div>
<p>We then find the row for ADJ and the column for NOUN and add one to this cell.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>ADJ</p></th>
<th class="head"><p>AUX</p></th>
<th class="head"><p>DET</p></th>
<th class="head"><p>NOUN</p></th>
<th class="head"><p>PRON</p></th>
<th class="head"><p>PROPN</p></th>
<th class="head"><p>VERB</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ADJ</p></td>
<td><p>2</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>AUX</p></td>
<td><p>0</p></td>
<td><p>2</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>DET</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>NOUN</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>PRON</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>PROPN</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>VERB</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p>As you can see, the correct predictions form a roughly diagonal line across the table.</p>
<p>We can use the table to derive two additional metrics for each class: <em>precision</em> and <em>recall</em>.</p>
<p>Precision is the <em>proportion of correct predictions per class</em>. In plain words, precision tells you how many predictions were correct for each class, or part-of-speech tag.</p>
<p>For example, the sum for column VERB is <span class="math notranslate nohighlight">\(2\)</span>, of which <span class="math notranslate nohighlight">\(1\)</span> prediction is correct (that which is located in the row VERB).</p>
<p>Hence precision for VERB is <span class="math notranslate nohighlight">\(1 / 2 = 0.5\)</span> – half of the tokens predicted to be verbs were classified correctly. The same holds true for NOUN, as the column sums up two <span class="math notranslate nohighlight">\(2\)</span>, but only <span class="math notranslate nohighlight">\(1\)</span> prediction is in the correct row.</p>
<p>Recall, in turn, gives the proportion of correct predictions for all examples of that class.</p>
<p>Put differently, recall tells you <em>how many actual instances of a given class the model was able to “find”</em>.</p>
<p>For example, the sum for row ADJ is <span class="math notranslate nohighlight">\(3\)</span>: there are three adjectives in the gold standard, but only two are located in the corresponding column for ADJ.</p>
<p>This means that recall for ADJ is <span class="math notranslate nohighlight">\(2 / 3 = 0.66\)</span> – approximately 66% of the adjectives present in the gold standard were classified correctly. For NOUN, recall is <span class="math notranslate nohighlight">\(1 / 2 = 0.5\)</span>.</p>
<p>The <em>scikit-learn</em> library provides a <code class="docutils literal notranslate"><span class="pre">confusion_matrix()</span></code> function for automatically generating confusion matrices.</p>
<p>Run the cell below and compare the output to the manually created confusion matrix above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate a confusion matrix for the two lists and print the result</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">gold_standard</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[2 0 0 1 0 0 0]
 [0 2 0 0 0 0 0]
 [0 0 1 0 0 0 0]
 [0 0 0 1 0 0 1]
 [0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1]]
</pre></div>
</div>
</div>
</div>
<p>To evaluate the ability of the language model to predict the correct part-of-speech tag, we can use <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score"><em>precision</em></a>, which is implemented in the <code class="docutils literal notranslate"><span class="pre">precision_score()</span></code> function in the <em>scikit-learn</em> library.</p>
<p>Because we have more than two classes for part-of-speech tags instead of just two (binary) classes, we must define how the results for each class are processed.</p>
<p>This option is set using the <code class="docutils literal notranslate"><span class="pre">average</span></code> argument of the <code class="docutils literal notranslate"><span class="pre">precision_score()</span></code> function. If we set <code class="docutils literal notranslate"><span class="pre">average</span></code> to <code class="docutils literal notranslate"><span class="pre">None</span></code>, the <code class="docutils literal notranslate"><span class="pre">precision_score()</span></code> function calculates precision for each class.</p>
<p>We also set the <code class="docutils literal notranslate"><span class="pre">zero_division</span></code> argument to tell the function what to do if the classes found in <code class="docutils literal notranslate"><span class="pre">predictions</span></code> are not present in <code class="docutils literal notranslate"><span class="pre">gold_standard</span></code>. This prevents calculating a precision score: <code class="docutils literal notranslate"><span class="pre">zero_division</span></code> sets the precision score to 0 in these cases.</p>
<p>The results are organised according to a sorted <em>set</em> of labels present in <code class="docutils literal notranslate"><span class="pre">gold_standard</span></code> and <code class="docutils literal notranslate"><span class="pre">predictions</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate precision between the two lists for each class (part-of-speech tag)</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">gold_standard</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Call the variable to examine the result</span>
<span class="n">precision</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1. , 1. , 1. , 0.5, 0. , 0. , 0.5])
</pre></div>
</div>
</div>
</div>
<p>The output is a <a class="reference external" href="https://www.numpy.org">NumPy</a> array. NumPy is a powerful library for working with numerical data which can be found under the hood of many Python libraries.</p>
<p>If we want to combine our list of labels in <code class="docutils literal notranslate"><span class="pre">pos_tags</span></code> with the precision scores in <code class="docutils literal notranslate"><span class="pre">precision</span></code>, we can do this using Python’s <code class="docutils literal notranslate"><span class="pre">zip()</span></code> function, which joins together lists and/or arrays of the same size. To view the result, we must cast it into a dictionary using <code class="docutils literal notranslate"><span class="pre">dict()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Combine the &#39;pos_tags&#39; set with the &#39;precision&#39; array using the zip()</span>
<span class="c1"># function; cast the result into a dictionary</span>
<span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">pos_tags</span><span class="p">,</span> <span class="n">precision</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ADJ&#39;: 1.0,
 &#39;AUX&#39;: 1.0,
 &#39;DET&#39;: 1.0,
 &#39;NOUN&#39;: 0.5,
 &#39;PRON&#39;: 0.0,
 &#39;PROPN&#39;: 0.0,
 &#39;VERB&#39;: 0.5}
</pre></div>
</div>
</div>
</div>
<p>If we want to get single precision score for all classes, we can use the option given by the string <code class="docutils literal notranslate"><span class="pre">'macro'</span></code>, which means that each class is treated as equally important regardless of how many instances belonging to this class can be found in the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate precision between the two lists and take their average</span>
<span class="n">macro_precision</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">gold_standard</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Call the variable to examine the result</span>
<span class="n">macro_precision</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5714285714285714
</pre></div>
</div>
</div>
</div>
<p>The macro-averaged precision score is calculated by summing up the precision scores and dividing them by the number of classes.</p>
<p>We can easily verify this manually.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate macro-average precision manually by summing the precision </span>
<span class="c1"># scores and dividing the result by the number of classes in &#39;precision&#39;</span>
<span class="nb">sum</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5714285714285714
</pre></div>
</div>
</div>
</div>
<p>Calculating recall is equally easy using the <code class="docutils literal notranslate"><span class="pre">recall_score()</span></code> function from the <em>scikit-learn</em> library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate recall between the two lists for each class (part-of-speech tag)</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">gold_standard</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Combine the &#39;pos_tags&#39; set with the &#39;recall&#39; array using the zip()</span>
<span class="c1"># function; cast the result into a dictionary</span>
<span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">pos_tags</span><span class="p">,</span> <span class="n">recall</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ADJ&#39;: 0.6666666666666666,
 &#39;AUX&#39;: 1.0,
 &#39;DET&#39;: 1.0,
 &#39;NOUN&#39;: 0.5,
 &#39;PRON&#39;: 0.0,
 &#39;PROPN&#39;: 0.0,
 &#39;VERB&#39;: 1.0}
</pre></div>
</div>
</div>
</div>
<p>The <em>scikit-learn</em> library provides a very useful function for providing an overview of classification performance called <code class="docutils literal notranslate"><span class="pre">classification_report()</span></code>.</p>
<p>This will give you the precision and recall scores for each class, together with the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">F1-score</a>, which is a balanced average of precision and recall, that is, both precision and recall contribute equally to the F1-score. The values for the F1-score run from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print out a classification report</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">gold_standard</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

         ADJ       1.00      0.67      0.80         3
         AUX       1.00      1.00      1.00         2
         DET       1.00      1.00      1.00         1
        NOUN       0.50      0.50      0.50         2
        PRON       0.00      0.00      0.00         1
       PROPN       0.00      0.00      0.00         0
        VERB       0.50      1.00      0.67         1

    accuracy                           0.70        10
   macro avg       0.57      0.60      0.57        10
weighted avg       0.75      0.70      0.71        10
</pre></div>
</div>
</div>
</div>
<p>As you can see, the macro-averaged scores on the row <em>macro avg</em> correspond to those that we calculated above.</p>
<p>Finally, the weighted averages account for the number of instances in each class when calculating the average. The column <em>support</em> counts the number of instances observed for each class.</p>
<p>This section should have given you an idea how to assess the reliability of human annotations, and how reliable annotations can be used as a gold standard for benchmarking the performance of natural language processing.</p>
<p>You should also understand certain basic metrics used for benchmarking performance, such as accuracy, precision, recall and F1-score.</p>
<p>In the <a class="reference internal" href="06_managing_data.html"><span class="doc std std-doc">following section</span></a>, you will learn about managing textual data.</p>
</div>
</div>


              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="04_basic_nlp_continued.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Customising the spaCy pipeline</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="06_managing_data.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Managing textual data using pandas</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

   <div class="footer"><a href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0;padding:10px" src="https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/by-nc.svg"/></a> © 2020– <a href="http://www.helsinki.fi/~thiippal">Tuomo Hiippala</a></div>
    

  </body>
</html>