
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QDXYHY09XJ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QDXYHY09XJ');
</script>
    
    
    <title>Evaluating language models</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Managing textual data using pandas" href="06_managing_data.html" />
    <link rel="prev" title="Customising the spaCy pipeline" href="04_basic_nlp_continued.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about.html">
   About this Website
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../getting_started.html">
   Getting Started
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notebook_login.html">
     Log in to CSC Notebooks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../launch_server.html">
     Launch JupyterLab on your personal server
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../jupyter.html">
     Interact with the server in JupyterLab
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../github_pull.html">
     Retrieve learning materials and exercises from GitHub
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../github_push.html">
     Return completed exercises to GitHub for grading
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../python_intro.html">
   Part I: A Minimal Introduction to Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_i/01_working_with_jupyter_notebooks.html">
     The elements of a Jupyter Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_i/02_getting_started_with_python.html">
     Getting started with Python
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../../working_with_text.html">
   Part II: Working with Text in Python
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01_basic_text_processing.html">
     Manipulating text using Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_basic_text_processing_continued.html">
     Manipulating text at scale
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_basic_nlp.html">
     Processing texts using spaCy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04_basic_nlp_continued.html">
     Customising the spaCy pipeline
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Evaluating language models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06_managing_data.html">
     Managing textual data using pandas
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../nlp_for_linguists.html">
   Part III: Natural Language Processing for Linguists
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_iii/01_multilingual_nlp.html">
     Processing diverse languages
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_iii/02_universal_dependencies.html">
     Universal Dependencies
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_iii/03_pattern_matching.html">
     Finding linguistic patterns using spaCy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_iii/04_embeddings.html">
     Introducing word embeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_iii/05_embeddings_continued.html">
     Word embeddings in spaCy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_iii/06_text_linguistics.html">
     Working with discourse-level annotations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../citation.html">
   Citation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../resources.html">
   Resources
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/notebooks/part_ii/05_evaluating_nlp.ipynb.txt"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Applied-Language-Technology/website"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/Applied-Language-Technology/website/binder?urlpath=lab/tree/notebooks/part_ii/05_evaluating_nlp.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-a-gold-standard">
   What is a gold standard?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#measuring-reliability-manually">
   Measuring reliability manually
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-1-annotate-data">
     Step 1: Annotate data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-2-calculate-percentage-agreement">
     Step 2: Calculate percentage agreement
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-3-calculate-probabilities-for-each-category">
     Step 3: Calculate probabilities for each category
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-4-estimate-chance-agreement">
     Step 4: Estimate chance agreement
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cohen-s-kappa-as-a-measure-of-agreement">
   Cohen‚Äôs kappa as a measure of agreement
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluating-the-performance-of-language-models">
   Evaluating the performance of language models
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="evaluating-language-models">
<h1>Evaluating language models<a class="headerlink" href="#evaluating-language-models" title="Permalink to this headline">¬∂</a></h1>
<p>This section introduces you to some basic techniques for evaluating the results of natural language processing.</p>
<p>After reading this section, you should:</p>
<ul class="simple">
<li><p>understand what is meant by a gold standard</p></li>
<li><p>know how to evaluate agreement between human annotators</p></li>
<li><p>understand simple metrics for evaluating the performance of natural language processing</p></li>
</ul>
<div class="section" id="what-is-a-gold-standard">
<h2>What is a gold standard?<a class="headerlink" href="#what-is-a-gold-standard" title="Permalink to this headline">¬∂</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe
    width="600"
    height="350"
    src="https://www.youtube.com/embed/eBJDxHUxRwc"
    frameborder="0"
    allowfullscreen

></iframe>
</div></div>
</div>
<p>A gold standard ‚Äì also called a ground truth ‚Äì refers to human-verified data that can used as a benchmark for evaluating the performance of algorithms.</p>
<p>In natural language processing, gold standards are used to measure how well humans perform on some task.</p>
<p>The goal of natural language processing is to allow computers to achieve or surpass human-level performance in some pre-defined task.</p>
<p>Measuring whether algorithms can do so requires a benchmark, which is provided by the gold standard. Put simply, a gold standard provides a point of reference.</p>
<p>It is important, however, to understand that gold standards are <em>abstractions</em> of language use.</p>
<p>Consider, for instance, the task of placing words into word classes: word classes are not given to us by nature, but represent an abstraction that imposes structure on natural language.</p>
<p>Language, however, is naturally ambiguous and subjective, and the abstractions used can be underdeveloped ‚Äì we cannot be sure if all users would categorise words in the same way.</p>
<p>This is why we need to measure the reliability of any gold standard, that is, to what extent humans agree on the task.</p>
</div>
<div class="section" id="measuring-reliability-manually">
<h2>Measuring reliability manually<a class="headerlink" href="#measuring-reliability-manually" title="Permalink to this headline">¬∂</a></h2>
<p>This section introduces how reliability, often understood as agreement between multiple annotators, can be measured manually.</p>
<div class="section" id="step-1-annotate-data">
<h3>Step 1: Annotate data<a class="headerlink" href="#step-1-annotate-data" title="Permalink to this headline">¬∂</a></h3>
<p>Sentiment analysis is a task that involves determining the sentiment of a text (for an useful overview that incorporates insights from both linguistics and natural language processing, see <a class="reference external" href="https://doi.org/10.1146/annurev-linguistics-011415-040518">Taboada</a> (2016).</p>
<p>Training a sentiment analysis model requires collecting training data, that is, examples of texts associated with different sentiments.</p>
<p>Classify the following tweets into three categories ‚Äì <em>positive</em>, <em>neutral</em> or <em>negative</em> ‚Äì based on their sentiment.</p>
<p>Write down your decision ‚Äì one per row ‚Äì but <strong>do not discuss them or show them to the person next to you.</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Updated: HSL GTFS (Helsinki, Finland) https://t.co/fWEpzmNQLz
2. current weather in Helsinki: broken clouds, -8¬∞C 100% humidity, wind 4kmh, pressure 1061mb
3. CNN: &quot;WallStreetBets Redditors go ballistic over GameStop&#39;s sinking share price&quot;
4. Baana bicycle counter. Today: 3 Same time last week: 1058 Trend: ‚Üì99% This year: 819 518 Last year: 802 079 #Helsinki #cycling
5. Elon Musk is now tweeting about #bitcoin 
6. A perfect Sunday walk in the woods just a few steps from home.
7. Went to Domino&#39;s todayüëç It was so amazing and I think I got damn good dessert as well‚Ä¶
8. Choo Choo üöÇ There&#39;s our train! üéâ #holidayahead
9. Happy women&#39;s day ‚ù§Ô∏èüíã kisses to all you beautiful ladies. üòö #awesometobeawoman
10. Good morning #Helsinki! Sun will rise in 30 minutes (local time 07:28)
</pre></div>
</div>
<p>Double-click this cell to edit its contents and write down your classifications below:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
</pre></div>
</div>
</div>
<div class="section" id="step-2-calculate-percentage-agreement">
<h3>Step 2: Calculate percentage agreement<a class="headerlink" href="#step-2-calculate-percentage-agreement" title="Permalink to this headline">¬∂</a></h3>
<p>When creating datasets for training models, we typically want the training data to be reliable, that is, so that we agree on whatever we are describing ‚Äì in this case, the sentiment of the tweets above.</p>
<p>One way to measure this is simple <em>percentage agreement</em>, that is, how many times out of 10 you and the person next to you agreed on the sentiment of a tweet.</p>
<p>Now compare your results calculate percentage agreement by dividing the number of times you agreed by the number of items (10).</p>
<p>You can calculate percentage agreement by executing the cell below: just assign the number items you agree on to the variable <code class="docutils literal notranslate"><span class="pre">agreement</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Replace this number here with the number of items you agreed on</span>
<span class="n">agreement</span> <span class="o">=</span> <span class="mi">0</span>  

<span class="c1"># Divide the count by the number of tweets</span>
<span class="n">agreement</span> <span class="o">=</span> <span class="n">agreement</span> <span class="o">/</span> <span class="mi">10</span>

<span class="c1"># Print out the variable</span>
<span class="n">agreement</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="step-3-calculate-probabilities-for-each-category">
<h3>Step 3: Calculate probabilities for each category<a class="headerlink" href="#step-3-calculate-probabilities-for-each-category" title="Permalink to this headline">¬∂</a></h3>
<p>Percentage agreement is actually a very poor measure of agreement, as either of you may have made lucky guesses ‚Äì or perhaps you considered the task boring and classified every tweet into a random category.</p>
<p>If you did, we have no way of knowing this, as percentage agreement cannot tell us if the result occurred by chance!</p>
<p>Luckily, we can estimate the possibility of <em>chance agreement</em> easily.</p>
<p>The first step is to count <em>how many times you used each available category</em> (positive, neutral or negative).</p>
<p>Assign these counts in the variables below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Count how many items *you* placed in each category</span>
<span class="n">positive</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">neutral</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">negative</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
<p>We can convert these counts into <em>probabilities</em> by dividing them with the total number of tweets classified.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">positive</span> <span class="o">=</span> <span class="n">positive</span> <span class="o">/</span> <span class="mi">10</span>
<span class="n">neutral</span> <span class="o">=</span> <span class="n">neutral</span> <span class="o">/</span> <span class="mi">10</span>
<span class="n">negative</span> <span class="o">=</span> <span class="n">negative</span> <span class="o">/</span> <span class="mi">10</span>

<span class="c1"># Call each variable to examine the output</span>
<span class="n">positive</span><span class="p">,</span> <span class="n">neutral</span><span class="p">,</span> <span class="n">negative</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.0, 0.0, 0.0)
</pre></div>
</div>
</div>
</div>
<p>These probabilities represent the chance of <em>you</em> choosing that particular category.</p>
<p>Now ask the person sitting next to you for their corresponding probabilities and tell them yours as well.</p>
<p>Add their probabilities to the variables below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb_positive</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">nb_neutral</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">nb_negative</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we know the probabilities for each class for both annotators, we can calculate the probability that both annotators choose the same category by chance.</p>
<p>This is easy: for each category, simply multiply your probability with the corresponding probability from the person next to you.</p>
<p>If either annotator did not assign a single tweet into a category, e.g. negative, and the other annotator did, then this effectively rules out the possibility of agreeing by chance (multiplication by zero results in zero).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">both_positive</span> <span class="o">=</span> <span class="n">positive</span> <span class="o">*</span> <span class="n">nb_positive</span>
<span class="n">both_neutral</span> <span class="o">=</span> <span class="n">neutral</span> <span class="o">*</span> <span class="n">nb_neutral</span>
<span class="n">both_negative</span> <span class="o">=</span> <span class="n">negative</span> <span class="o">*</span> <span class="n">nb_negative</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="step-4-estimate-chance-agreement">
<h3>Step 4: Estimate chance agreement<a class="headerlink" href="#step-4-estimate-chance-agreement" title="Permalink to this headline">¬∂</a></h3>
<p>Now we are ready to calculate how likely you are to agree by chance.</p>
<p>This is known as <em>expected agreement</em>, which is calculated by summing up your combined probabilities for each category.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">expected_agreement</span> <span class="o">=</span> <span class="n">both_positive</span> <span class="o">+</span> <span class="n">both_neutral</span> <span class="o">+</span> <span class="n">both_negative</span>

<span class="n">expected_agreement</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
</div>
</div>
<p>Now that we know both observed percentage agreement (<code class="docutils literal notranslate"><span class="pre">agreement</span></code>) and the agreement expected by chance (<code class="docutils literal notranslate"><span class="pre">expected_agreement</span></code>), we can use this information for a more reliable measure of <em>agreement</em>.</p>
<p>One such measure is <a class="reference external" href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen‚Äôs kappa</a> (<span class="math notranslate nohighlight">\(\kappa\)</span>), which estimates agreement on the basis of both observed and expected agreement.</p>
<p>The formula for Cohen‚Äôs <span class="math notranslate nohighlight">\(\kappa\)</span> is as follows:</p>
<p><span class="math notranslate nohighlight">\(\kappa = \frac{P_{observed} - P_{expected}}{1 - P_{expected}}\)</span></p>
<p>As all this information is stored in our variables <code class="docutils literal notranslate"><span class="pre">agreement</span></code> and <code class="docutils literal notranslate"><span class="pre">expected_agreement</span></code>, we can easily count the <span class="math notranslate nohighlight">\(\kappa\)</span> score using the code below.</p>
<p>Note that we must wrap the subtractions into parentheses to perform them before division.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kappa</span> <span class="o">=</span> <span class="p">(</span><span class="n">agreement</span> <span class="o">-</span> <span class="n">expected_agreement</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">expected_agreement</span><span class="p">)</span>

<span class="n">kappa</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
</div>
</div>
<p>This gives us the result for Cohen‚Äôs <span class="math notranslate nohighlight">\(\kappa\)</span>.</p>
<p>Let‚Äôs now consider how to interpret its value.</p>
</div>
</div>
<div class="section" id="cohen-s-kappa-as-a-measure-of-agreement">
<h2>Cohen‚Äôs kappa as a measure of agreement<a class="headerlink" href="#cohen-s-kappa-as-a-measure-of-agreement" title="Permalink to this headline">¬∂</a></h2>
<p>The theoretical value Cohen‚Äôs <span class="math notranslate nohighlight">\(\kappa\)</span> runs from <span class="math notranslate nohighlight">\(-1\)</span> indicating perfect disagreement to <span class="math notranslate nohighlight">\(+1\)</span> for perfect agreement, with <span class="math notranslate nohighlight">\(0\)</span> standing for completely random agreement.</p>
<p>The <span class="math notranslate nohighlight">\(\kappa\)</span> score is often interpreted as a measure of the strength of agreement.</p>
<p><a class="reference external" href="https://doi.org/10.2307/2529310">Landis and Koch</a> (1977) famously proposed the following benchmarks, which should nevertheless be taken with a pinch of salt as the divisions are completely arbitrary.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Cohen‚Äôs K</p></th>
<th class="head"><p>Strength of agreement</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>&lt;0.00</p></td>
<td><p>Poor</p></td>
</tr>
<tr class="row-odd"><td><p>0.00‚Äì0.20</p></td>
<td><p>Slight</p></td>
</tr>
<tr class="row-even"><td><p>0.21‚Äì0.40</p></td>
<td><p>Fair</p></td>
</tr>
<tr class="row-odd"><td><p>0.41‚Äì0.60</p></td>
<td><p>Moderate</p></td>
</tr>
<tr class="row-even"><td><p>0.61‚Äì0.80</p></td>
<td><p>Substantial</p></td>
</tr>
<tr class="row-odd"><td><p>0.81‚Äì1.00</p></td>
<td><p>Almost perfect</p></td>
</tr>
</tbody>
</table>
<p>Cohen‚Äôs <span class="math notranslate nohighlight">\(\kappa\)</span> can be used to measure agreement between <strong>two</strong> annotators and the categories available must be <strong>fixed</strong> in advance.</p>
<p>For measuring agreement between more than two annotators, one must use a measure such as <a class="reference external" href="https://en.wikipedia.org/wiki/Fleiss%27_kappa">Fleiss‚Äô</a> <span class="math notranslate nohighlight">\(\kappa\)</span>.</p>
<p>Cohen‚Äôs <span class="math notranslate nohighlight">\(\kappa\)</span> and many more measures of agreement are implemented in various Python libraries, so one rarely needs to perform the calculations manually.</p>
<p>The <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score">scikit-learn</a> library (<code class="docutils literal notranslate"><span class="pre">sklearn</span></code>), for instance, includes an implementation of Cohen‚Äôs <span class="math notranslate nohighlight">\(\kappa\)</span>.</p>
<p>Let‚Äôs import the <code class="docutils literal notranslate"><span class="pre">cohen_kappa_score()</span></code> function for calculating Cohen‚Äôs <span class="math notranslate nohighlight">\(\kappa\)</span> from scikit-learn.</p>
<p>This function takes two <em>lists</em> as input and calculates the <span class="math notranslate nohighlight">\(\kappa\)</span> score between them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the cohen_kappa_score function from the &#39;metrics&#39; module of the scikit-learn library</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">cohen_kappa_score</span>
</pre></div>
</div>
</div>
</div>
<p>We can then define two lists of part-of-speech tags, which make up our toy example for calculating Cohen‚Äôs <span class="math notranslate nohighlight">\(\kappa\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define two lists named &#39;a1&#39; and &#39;a2&#39;</span>
<span class="n">a1</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;AUX&#39;</span><span class="p">,</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">]</span>
<span class="n">a2</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">,</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The next step is to feed the two lists, <code class="docutils literal notranslate"><span class="pre">a1</span></code> and <code class="docutils literal notranslate"><span class="pre">a2</span></code>, to the <code class="docutils literal notranslate"><span class="pre">cohen_kappa_score()</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use the cohen_kappa_score() function to calculate agreement between the lists</span>
<span class="n">cohen_kappa_score</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.44444444444444453
</pre></div>
</div>
</div>
</div>
<p>According to the benchmark from Landis and Koch, this score would indicate moderate agreement.</p>
<p>Generally, Cohen‚Äôs <span class="math notranslate nohighlight">\(\kappa\)</span> can be used for measuring agreement on all kinds of tasks that involve placing items into categories.</p>
<p>It is rarely necessary to annotate the whole dataset when measuring agreement ‚Äì a random sample is often enough.</p>
<p>If Cohen‚Äôs <span class="math notranslate nohighlight">\(\kappa\)</span> suggests that the human annotators agree on whatever they are categorising, we can assume that the annotations are <em>reliable</em> in the sense that they are not random.</p>
<p>However, all measures of inter-annotator agreement, Cohen‚Äôs <span class="math notranslate nohighlight">\(\kappa\)</span> included, are affected by their underlying assumptions about what agreement is and how it is calculated. In other words, these measures never represent the absolute truth (see e.g. Di Eugenio &amp; Glass <a class="reference external" href="https://dx.doi.org/10.1162/089120104773633402">2004</a>; Artstein &amp; Poesio <a class="reference external" href="https://doi.org/10.1162/coli.07-034-R2">2008</a>).</p>
</div>
<div class="section" id="evaluating-the-performance-of-language-models">
<h2>Evaluating the performance of language models<a class="headerlink" href="#evaluating-the-performance-of-language-models" title="Permalink to this headline">¬∂</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe
    width="600"
    height="350"
    src="https://www.youtube.com/embed/WiN5JCueeFQ"
    frameborder="0"
    allowfullscreen

></iframe>
</div></div>
</div>
<p>Once we have a sufficiently <em>reliable</em> gold standard, we can use the gold standard to measure the performance of language models.</p>
<p>Let‚Äôs assume that we have a reliable gold standard consisting of 10 tokens annotated for their part-of-speech tags.</p>
<p>These part-of-speech tags are given in the list <code class="docutils literal notranslate"><span class="pre">gold_standard</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a list named &#39;gold_standard&#39;</span>
<span class="n">gold_standard</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;AUX&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">,</span> <span class="s1">&#39;AUX&#39;</span><span class="p">,</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;DET&#39;</span><span class="p">,</span> <span class="s1">&#39;PRON&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We then retrieve the predictions for the same tokens from some language model and store them in a list named <code class="docutils literal notranslate"><span class="pre">predictions</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a list named &#39;predictions&#39;</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;AUX&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">,</span> <span class="s1">&#39;AUX&#39;</span><span class="p">,</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">,</span> <span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;DET&#39;</span><span class="p">,</span> <span class="s1">&#39;PROPN&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have a toy data set with two sets of annotations to compare, let‚Äôs import the entire <em>metrics</em> module from the <em>scikit-learn</em> library and apply them to our data.</p>
<p>This module contains implementations for <a class="reference external" href="https://scikit-learn.org/stable/modules/model_evaluation.html">various evaluation metrics</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the &#39;metrics&#39; module from the scikit-learn library (sklearn)</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
</pre></div>
</div>
</div>
</div>
<p>First of all, we can calculate <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score">accuracy</a> using the <code class="docutils literal notranslate"><span class="pre">accuracy_score()</span></code> function, which is precisely the same as observed agreement that we calculated manually above.</p>
<p>This function takes two lists as input.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use the accuracy_score() function from the &#39;metrics&#39; module</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">gold_standard</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7
</pre></div>
</div>
</div>
</div>
<p>Accuracy, however, suffers from the same shortcoming as observed agreement ‚Äì the output of the language model in <code class="docutils literal notranslate"><span class="pre">predictions</span></code> may be the result of making a series of lucky guesses.</p>
<p>However, given that we are working with a toy example, we can easily verify that 7 out of 10 part-of-speech tags match. This gives an accuracy of 0.7 or 70%.</p>
<p>To better evaluate the performance of the language model against our gold standard, the results can be organised into what is called a <em>confusion matrix</em>.</p>
<p>To do so, we need all part-of-speech tags that occur in <code class="docutils literal notranslate"><span class="pre">gold_standard</span></code> and <code class="docutils literal notranslate"><span class="pre">predictions</span></code>.</p>
<p>We can easily collect unique part-of-speech tags using the <code class="docutils literal notranslate"><span class="pre">set()</span></code> function.</p>
<p>The result is a <em>set</em>, a powerful data structure in Python, which consists of a collection of unique items.</p>
<p>Essentially, we use a set to remove duplicates in the two lists <code class="docutils literal notranslate"><span class="pre">gold_standard</span></code> and <code class="docutils literal notranslate"><span class="pre">predictions</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Collect unique POS tags into a set by combining the two lists</span>
<span class="n">pos_tags</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">gold_standard</span> <span class="o">+</span> <span class="n">predictions</span><span class="p">)</span>

<span class="c1"># Sort the set alphabetically and cast the result into a list</span>
<span class="n">pos_tags</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">pos_tags</span><span class="p">))</span>

<span class="c1"># Print the resulting list</span>
<span class="n">pos_tags</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;ADJ&#39;, &#39;AUX&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;PRON&#39;, &#39;PROPN&#39;, &#39;VERB&#39;]
</pre></div>
</div>
</div>
</div>
<p>We can use these unique categories to compile a table, in which the rows stand for the gold standard and the columns stand for predictions made by the language model. Having collected all unique part-of-speech tags at hand ensures that we can always find a place for each item.</p>
<p>This kind of table is commonly called a <em>confusion matrix</em>.</p>
<p>The table is populated by simply walking through each pair of items in the gold standard and model predictions, adding <span class="math notranslate nohighlight">\(+1\)</span> to the cell for this combination.</p>
<p>For example, the first item in <code class="docutils literal notranslate"><span class="pre">gold_standard</span></code> is ADJ, whereas the first item in <code class="docutils literal notranslate"><span class="pre">predictions</span></code> is NOUN.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print out the first item in each list</span>
<span class="n">gold_standard</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&#39;ADJ&#39;, &#39;NOUN&#39;)
</pre></div>
</div>
</div>
</div>
<p>We then find the row for ADJ and the column for NOUN and add one to this cell.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>ADJ</p></th>
<th class="head"><p>AUX</p></th>
<th class="head"><p>DET</p></th>
<th class="head"><p>NOUN</p></th>
<th class="head"><p>PRON</p></th>
<th class="head"><p>PROPN</p></th>
<th class="head"><p>VERB</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ADJ</p></td>
<td><p>2</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>AUX</p></td>
<td><p>0</p></td>
<td><p>2</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>DET</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>NOUN</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>PRON</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>PROPN</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>VERB</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p>As you can see, the correct predictions form a roughly diagonal line across the table.</p>
<p>We can use the table to derive two additional metrics for each class: <em>precision</em> and <em>recall</em>.</p>
<p>Precision is the <em>proportion of correct predictions per class</em>. In plain words, precision tells you how many predictions were correct for each class, or part-of-speech tag.</p>
<p>For example, the sum for column VERB is <span class="math notranslate nohighlight">\(2\)</span>, of which <span class="math notranslate nohighlight">\(1\)</span> prediction is correct (that which is located in the row VERB).</p>
<p>Hence precision for VERB is <span class="math notranslate nohighlight">\(1 / 2 = 0.5\)</span> ‚Äì half of the tokens predicted to be verbs were classified correctly. The same holds true for NOUN, as the column sums up two <span class="math notranslate nohighlight">\(2\)</span>, but only <span class="math notranslate nohighlight">\(1\)</span> prediction is in the correct row.</p>
<p>Recall, in turn, gives the proportion of correct predictions for all examples of that class.</p>
<p>Put differently, recall tells you <em>how many actual instances of a given class the model was able to ‚Äúfind‚Äù</em>.</p>
<p>For example, the sum for row ADJ is <span class="math notranslate nohighlight">\(3\)</span>: there are three adjectives in the gold standard, but only two are located in the corresponding column for ADJ.</p>
<p>This means that recall for ADJ is <span class="math notranslate nohighlight">\(2 / 3 = 0.66\)</span> ‚Äì approximately 66% of the adjectives present in the gold standard were classified correctly. For NOUN, recall is <span class="math notranslate nohighlight">\(1 / 2 = 0.5\)</span>.</p>
<p>The <em>scikit-learn</em> library provides a <code class="docutils literal notranslate"><span class="pre">confusion_matrix()</span></code> function for automatically generating confusion matrices.</p>
<p>Run the cell below and compare the output to the manually created confusion matrix above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate a confusion matrix for the two lists and print the result</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">gold_standard</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[2 0 0 1 0 0 0]
 [0 2 0 0 0 0 0]
 [0 0 1 0 0 0 0]
 [0 0 0 1 0 0 1]
 [0 0 0 0 0 1 0]
 [0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1]]
</pre></div>
</div>
</div>
</div>
<p>To evaluate the ability of the language model to predict the correct part-of-speech tag, we can use <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score"><em>precision</em></a>, which is implemented in the <code class="docutils literal notranslate"><span class="pre">precision_score()</span></code> function in the <em>scikit-learn</em> library.</p>
<p>Because we have more than two classes for part-of-speech tags instead of just two (binary) classes, we must define how the results for each class are processed.</p>
<p>This option is set using the <code class="docutils literal notranslate"><span class="pre">average</span></code> argument of the <code class="docutils literal notranslate"><span class="pre">precision_score()</span></code> function. If we set <code class="docutils literal notranslate"><span class="pre">average</span></code> to <code class="docutils literal notranslate"><span class="pre">None</span></code>, the <code class="docutils literal notranslate"><span class="pre">precision_score()</span></code> function calculates precision for each class.</p>
<p>We also set the <code class="docutils literal notranslate"><span class="pre">zero_division</span></code> argument to tell the function what to do if the classes found in <code class="docutils literal notranslate"><span class="pre">predictions</span></code> are not present in <code class="docutils literal notranslate"><span class="pre">gold_standard</span></code>. This prevents calculating a precision score: <code class="docutils literal notranslate"><span class="pre">zero_division</span></code> sets the precision score to 0 in these cases.</p>
<p>The results are organised according to a sorted <em>set</em> of labels present in <code class="docutils literal notranslate"><span class="pre">gold_standard</span></code> and <code class="docutils literal notranslate"><span class="pre">predictions</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate precision between the two lists for each class (part-of-speech tag)</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">gold_standard</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Call the variable to examine the result</span>
<span class="n">precision</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1. , 1. , 1. , 0.5, 0. , 0. , 0.5])
</pre></div>
</div>
</div>
</div>
<p>The output is a <a class="reference external" href="https://www.numpy.org">NumPy</a> array. NumPy is a powerful library for working with numerical data which can be found under the hood of many Python libraries.</p>
<p>If we want to combine our list of labels in <code class="docutils literal notranslate"><span class="pre">pos_tags</span></code> with the precision scores in <code class="docutils literal notranslate"><span class="pre">precision</span></code>, we can do this using Python‚Äôs <code class="docutils literal notranslate"><span class="pre">zip()</span></code> function, which joins together lists and/or arrays of the same size. To view the result, we must cast it into a dictionary using <code class="docutils literal notranslate"><span class="pre">dict()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Combine the &#39;pos_tags&#39; set with the &#39;precision&#39; array using the zip()</span>
<span class="c1"># function; cast the result into a dictionary</span>
<span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">pos_tags</span><span class="p">,</span> <span class="n">precision</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ADJ&#39;: 1.0,
 &#39;AUX&#39;: 1.0,
 &#39;DET&#39;: 1.0,
 &#39;NOUN&#39;: 0.5,
 &#39;PRON&#39;: 0.0,
 &#39;PROPN&#39;: 0.0,
 &#39;VERB&#39;: 0.5}
</pre></div>
</div>
</div>
</div>
<p>If we want to get single precision score for all classes, we can use the option given by the string <code class="docutils literal notranslate"><span class="pre">'macro'</span></code>, which means that each class is treated as equally important regardless of how many instances belonging to this class can be found in the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate precision between the two lists and take their average</span>
<span class="n">macro_precision</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">gold_standard</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Call the variable to examine the result</span>
<span class="n">macro_precision</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5714285714285714
</pre></div>
</div>
</div>
</div>
<p>The macro-averaged precision score is calculated by summing up the precision scores and dividing them by the number of classes.</p>
<p>We can easily verify this manually.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate macro-average precision manually by summing the precision </span>
<span class="c1"># scores and dividing the result by the number of classes in &#39;precision&#39;</span>
<span class="nb">sum</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5714285714285714
</pre></div>
</div>
</div>
</div>
<p>Calculating recall is equally easy using the <code class="docutils literal notranslate"><span class="pre">recall_score()</span></code> function from the <em>scikit-learn</em> library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate recall between the two lists for each class (part-of-speech tag)</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">gold_standard</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Combine the &#39;pos_tags&#39; set with the &#39;recall&#39; array using the zip()</span>
<span class="c1"># function; cast the result into a dictionary</span>
<span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">pos_tags</span><span class="p">,</span> <span class="n">recall</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ADJ&#39;: 0.6666666666666666,
 &#39;AUX&#39;: 1.0,
 &#39;DET&#39;: 1.0,
 &#39;NOUN&#39;: 0.5,
 &#39;PRON&#39;: 0.0,
 &#39;PROPN&#39;: 0.0,
 &#39;VERB&#39;: 1.0}
</pre></div>
</div>
</div>
</div>
<p>The <em>scikit-learn</em> library provides a very useful function for providing an overview of classification performance called <code class="docutils literal notranslate"><span class="pre">classification_report()</span></code>.</p>
<p>This will give you the precision and recall scores for each class, together with the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">F1-score</a>, which is a balanced average of precision and recall, that is, both precision and recall contribute equally to the F1-score. The values for the F1-score run from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print out a classification report</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">gold_standard</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

         ADJ       1.00      0.67      0.80         3
         AUX       1.00      1.00      1.00         2
         DET       1.00      1.00      1.00         1
        NOUN       0.50      0.50      0.50         2
        PRON       0.00      0.00      0.00         1
       PROPN       0.00      0.00      0.00         0
        VERB       0.50      1.00      0.67         1

    accuracy                           0.70        10
   macro avg       0.57      0.60      0.57        10
weighted avg       0.75      0.70      0.71        10
</pre></div>
</div>
</div>
</div>
<p>As you can see, the macro-averaged scores on the row <em>macro avg</em> correspond to those that we calculated above.</p>
<p>Finally, the weighted averages account for the number of instances in each class when calculating the average. The column <em>support</em> counts the number of instances observed for each class.</p>
<p>This section should have given you an idea how to assess the reliability of human annotations, and how reliable annotations can be used as a gold standard for benchmarking the performance of natural language processing.</p>
<p>You should also understand certain basic metrics used for benchmarking performance, such as accuracy, precision, recall and F1-score.</p>
<p>In the <a class="reference internal" href="06_managing_data.html"><span class="doc std std-doc">following section</span></a>, you will learn about managing textual data.</p>
</div>
</div>


              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="04_basic_nlp_continued.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Customising the spaCy pipeline</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="06_managing_data.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Managing textual data using pandas</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

   <div class="footer"><a href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0;padding:10px" src="https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/by-nc.svg"/></a> ¬© 2020‚Äì <a href="http://www.helsinki.fi/~thiippal">Tuomo Hiippala</a></div>
    

  </body>
</html>